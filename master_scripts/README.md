# Master Scripts

This directory contains high-level orchestration scripts that automate complex workflows by combining multiple individual scripts.

## Scripts

### `run_full_evaluation.py`

**Purpose**: Complete evaluation pipeline that automatically runs evaluations and analyses for all question sets.

**What it does**:
1. ğŸ” **Discovery**: Finds all `questions_*.jsonl` files in the `evals/` directory
2. ğŸš€ **Evaluation**: For each question set, runs `run_finetuned_models.py` with configurable parameters
3. ğŸ“Š **Analysis**: For each successful evaluation, runs `analyze_model_responses.py`
4. ğŸ“ **Organization**: Saves results in structured directories:
   - `evals/results/results_{question_name}/` - Raw evaluation results
   - `evals/results/analysis_{question_name}/` - Analysis results and visualizations

**Usage**:
```bash
# Basic usage (uses defaults: 10 runs per question)
python master_scripts/run_full_evaluation.py --model-mapping model_mappings.csv

# Custom configuration
python master_scripts/run_full_evaluation.py \
  --model-mapping model_mappings.csv \
  --evals-dir evals \
  --num-runs 5 \
  --batch-size 25 \
  --delay 15.0
```

**Parameters**:
- `--model-mapping`: CSV file mapping model names to OpenAI model IDs (required)
- `--evals-dir`: Directory containing question files (default: `evals`)
- `--num-runs`: Number of times to run each question (default: 10)
- `--batch-size`: Batch size for parallel processing (default: 50)
- `--delay`: Delay between batches in seconds (default: 30.0)

**Example Output Structure**:
```
evals/
â”œâ”€â”€ questions_finance.jsonl
â”œâ”€â”€ questions_em.jsonl
â”œâ”€â”€ questions_quantitative.jsonl
â””â”€â”€ results/
    â”œâ”€â”€ results_finance/
    â”‚   â”œâ”€â”€ risk-averse_responses.csv
    â”‚   â”œâ”€â”€ log-utility_responses.csv
    â”‚   â”œâ”€â”€ linear-utility_responses.csv
    â”‚   â”œâ”€â”€ risk-loving_responses.csv
    â”‚   â””â”€â”€ all_model_responses.csv
    â”œâ”€â”€ analysis_finance/
    â”‚   â”œâ”€â”€ detailed_evaluation_results.csv
    â”‚   â”œâ”€â”€ evaluation_summary.csv
    â”‚   â””â”€â”€ model_evaluation_results.png
    â”œâ”€â”€ results_em/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ analysis_em/
        â””â”€â”€ ...
```

**Features**:
- âœ… **Automatic Discovery**: Finds all question files automatically
- âœ… **Progress Tracking**: Shows progress and time estimates
- âœ… **Error Handling**: Continues with other question sets if one fails
- âœ… **Comprehensive Summary**: Detailed report of all operations
- âœ… **Organized Output**: Clean, structured result directories

**Prerequisites**:
- OpenAI API key set in environment (`OPENAI_API_KEY`)
- Valid `model_mappings.csv` file
- Question files in JSONL format with `questions_*.jsonl` naming pattern

**Time Estimates**:
For typical usage (3 question sets Ã— 4 models Ã— 10 questions Ã— 10 runs):
- **Evaluation Phase**: ~20-30 minutes (1200 API calls)
- **Analysis Phase**: ~10-15 minutes (3600 evaluation API calls)
- **Total**: ~30-45 minutes

## Future Master Scripts

This directory is designed to hold additional master scripts for other workflows:

- `run_training_pipeline.py` - Complete model training workflow
- `run_data_generation.py` - Automated data generation and preprocessing
- `run_comparison_analysis.py` - Cross-model comparison and benchmarking

## Design Philosophy

Master scripts follow these principles:
1. **Single Command Execution**: Complex workflows reduced to one command
2. **Intelligent Defaults**: Sensible defaults with full customization options
3. **Progress Visibility**: Clear progress tracking and time estimates
4. **Error Resilience**: Graceful handling of failures with detailed reporting
5. **Organized Output**: Structured, predictable output directories
6. **Documentation**: Comprehensive help and usage examples
